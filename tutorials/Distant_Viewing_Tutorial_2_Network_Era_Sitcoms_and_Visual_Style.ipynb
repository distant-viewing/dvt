{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distant Viewing Tutorial 2: Network-Era Sitcoms and Visual Style\n",
        "\n",
        "This notebook explores the theory and methods\n",
        "introduced in the book *Distant Viewing* (MIT Press, 2023) to study visual\n",
        "style in two network era sitcoms. Specifically, we will look at every televised\n",
        "episode of the series *Bewitched* (1964-1972) and *I Dream of Jeannie*\n",
        "(1965-1970). In the notebook we will first walk through the methodology using a\n",
        "short 45 second sample video and going slowly through all of the steps. Then,\n",
        "due to time, file size, and copyright constraints, we will load a precomputed\n",
        "set of image annotations mirroring those from the 45 second sample and then\n",
        "use this larger set for the purpose of analysis. For a more complete analysis\n",
        "of these data, please see the original article [\"Visual Style in Two Network Era\n",
        "Sitcoms\"](https://distantviewing.org/papers/2021-visual-style-two-network-era-sitcoms.pdf) (2019, *Cultural Analytics*)\n",
        "and the fifth chapter of the [*Distant Viewing*](https://www.distantviewing.org/book/)\n",
        "book. Both of these are available under Creative Commons Open Access licenses.\n",
        "\n",
        "This notebook does not require any previous knowledge of Python or computer\n",
        "vision. However, it moves fairly quickly through the preliminary steps of\n",
        "working with digital images and only explains the most important aspects of\n",
        "the Python code in each step. For a more in-depth introduction to how computers view images images  and python, we recommend first following the\n",
        " Distant Viewing Tutorial: Movie Posters and Color Analysis notebook using the movie posters corpus, which\n",
        "can be accessed [here](https://colab.research.google.com/drive/1qQKQw8qHsTG7mK7Rz-z8nBfl98QBMWGf?usp=sharing). For more about the Python package that we built, Distant Viewing Toolkit (**dvt**), please visit [our GitHub repository](https://github.com/distant-viewing/dvt).\n",
        "\n"
      ],
      "metadata": {
        "id": "yyLWcbfjuu8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Setup\n",
        "\n",
        "As a first step, we need to install a few additional Python components,\n",
        "download the datasets, and tell Python all of the functions\n",
        "that we will need later in the tutorial. To get started, we will use the\n",
        "code below to install the module called **dvt** (the distant viewing toolkit),\n",
        "which contains several useful functions specifically designed to apply computer\n",
        "vision algorithms to collections of humanities data. The exclamation point at\n",
        "the start of the line of code tells the notebook that we we want to directly run a command line tool outside of Python itself. Here, we are using the tool\n",
        "called **pip** that can used to install additional functionality for Python.\n",
        "To run the code, hover your mouse somewhere over the background of the\n",
        "code. This will show a triangular play button on the left-hand side of the code\n",
        "block. Hit the button and wait for it to finish, which may take a minute or\n",
        "two as Colab always takes a bit of time to set up when running the first\n",
        "code block."
      ],
      "metadata": {
        "id": "VmYU2R8qzyXy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT2DBTkqN57u"
      },
      "outputs": [],
      "source": [
        "!pip install -q dvt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, below we have some code to download the metadata and movie posters that\n",
        "we are working with in this notebook. These use the program **wget** to download\n",
        "files from the distant viewing website followed by the **tar** command to\n",
        "unzip the directory containing the movie poster thumbnails. We will explore the\n",
        "individual contents of each of these components as needed in the sections below.\n",
        "The last line uses **mkdir** to make an empty directory that will be needed in\n",
        "the final section of the notebook for running machine learning models over the\n",
        "collection. As above, hover over the code and then click on the play button on\n",
        "the left-hand side of the code block. This will run all of the lines of code\n",
        "one after another. We should be able to see a small green arrow showing the\n",
        "specific line being run at any given point, helping us understand which steps\n",
        "take the most time to execute."
      ],
      "metadata": {
        "id": "xGJ6eVS5z-XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -nc \"https://distantviewing.org/tutorial/bewitched_sample.mp4\"\n",
        "!wget -q -nc \"https://distantviewing.org/tutorial/faces.tar\"\n",
        "!wget -q -nc \"https://distantviewing.org/tutorial/ttl_sitcom_metadata.csv\"\n",
        "!wget -q -nc \"https://distantviewing.org/tutorial/ttl_sitcom_characters.csv\"\n",
        "!wget -q -nc \"https://distantviewing.org/tutorial/ttl_sitcom_shots.csv\"\n",
        "!tar xf faces.tar\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints/"
      ],
      "metadata": {
        "id": "xMK8wuwrOAuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last part of the setup, we will run some code in Python (note that the\n",
        "lines below do not start with an exclamation mark). This code here uses\n",
        "the **import** command to tell Python which libraries and functions we are\n",
        "going to use in the notebook. Specifically, these are **os** for working with\n",
        "files on the operating system, **numpy** for working with large arrays of\n",
        "numbers, **pandas** for working with tabular datasets,\n",
        "**matplotlib.pyplot** for data visualization, **moviepy.editor** for displaying\n",
        "video files, and the **dvt** module that we described above."
      ],
      "metadata": {
        "id": "R5-NQa0i0BWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import dvt\n",
        "import moviepy.editor\n",
        "from google.colab.patches import cv2"
      ],
      "metadata": {
        "id": "kRSmSwXwQE3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Google Colab, your working environment resets itself everytime you reopen a\n",
        "notebook. Therefore, all of the steps above need to be re-run each time that\n",
        "you start the notebook. If you were running this code on your own machine, the\n",
        "installation of the **dvt** package and downloading the data would only need to\n",
        "be done once. Loading the modules in the final code chunk, however, always needs\n",
        "to be run each time that Python is restarted."
      ],
      "metadata": {
        "id": "saFiOCQR027w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Network-Era Sitcom Dataset\n",
        "\n",
        "Before we start looking at the computational steps needed to work with moving\n",
        "image data, it's helpful to understand the humanities research questions that\n",
        "motivate this work. Here, we are concerned with two popular U.S. sitcoms from\n",
        "the 1960s and early 1970s: *Bewitched* (1964-1972) and *I Dream of Jeannie*\n",
        "(1965-1970). Here is metadata abotu each of the episodes from the entire runs\n",
        "of the two shows:"
      ],
      "metadata": {
        "id": "Mn2KaGvU9Ih0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta = pd.read_csv(\"ttl_sitcom_metadata.csv\")\n",
        "meta"
      ],
      "metadata": {
        "id": "olv0GMe69OLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two series are often compared and constrasted to one another, with\n",
        "*I Dream of Jeannie* being seen as an attempt by NBC to copy the success of\n",
        "ABC's *Bewitched*, which started one season prior. While a lot has been written\n",
        "about the cultural significance of these two shows, prior research had not\n",
        "seriously considered the visual style of the two shows. We are interested in\n",
        "how the way that the shows are shot and edited contribute to our understanding\n",
        "of questions such as who is/are the main characters, to what extent does the\n",
        "visual style contribute to relationships between the characters, and how do\n",
        "these aspects change over time. The latter is particularly interesting because\n",
        "both shows started their broadcasts in black-and-white before transitioning\n",
        "to color for the majority of the later seasons."
      ],
      "metadata": {
        "id": "GRDeQO8P-uSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Sample Movie File\n",
        "\n",
        "Now that we have some ideas of the big questions we are interested in, let's\n",
        "look at a short sample from one of the series to understand the computational\n",
        "steps needed to work with moving images. When doing computational analyses with\n",
        "visual and audiovisual materials, it is\n",
        "important to frequently go back and look at specific examples to\n",
        "ensure that we understand how our large-scale analysis connects back to the\n",
        "actual human experience of viewing. In this notebook we are going to start by\n",
        "working slowly with the process of creating annotations that summarize a short\n",
        "45 second clip from the first episode of the third season of the show\n",
        "*Bewitched*. Running the code below shows an embedded version of the\n",
        "clip that can be watched within the Colab notebook."
      ],
      "metadata": {
        "id": "mWEEe4lb1RNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moviepy.editor.ipython_display(\"bewitched_sample.mp4\")"
      ],
      "metadata": {
        "id": "vQ570BKkOgfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We suggest watching the clip a couple of times. Try to pay attention (and maybe\n",
        "even write down) the number of shots in the clip and how they are framed. In\n",
        "the next few sections, we will use computer vision algorithms to try to\n",
        "capture these features as structured annotations."
      ],
      "metadata": {
        "id": "vtKfcU4B3j1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Working with Video Frames as Images\n",
        "\n",
        "Video file formats such as **mp4**, **avi**, and **ogg** typically store moving\n",
        "images in a complex format that is highly optimized to reduce file sizes and\n",
        "decompression time. When working with moving images computationally in programs\n",
        "such as Python, however, we typically decompress these files and turn them\n",
        "into a sequence of individual images stored as arrays of pixels, with one\n",
        "image for each frame in the input video. The distant viewing toolkit  (**dvt**) contains\n",
        "several functions to efficently work with video files in Python. In this\n",
        "section, we will demonstrate how these work.\n",
        "\n",
        "First, we can use the `video_info` function to access metadata about a video\n",
        "file. Here, we will load the metadata for our sample video file, which displays\n",
        "the number of frames, the height and width of each frame in pixels, and rate\n",
        "of playback given as frames per second (`fps`)."
      ],
      "metadata": {
        "id": "t-pgFEL_1WcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = dvt.video_info('bewitched_sample.mp4')\n",
        "info"
      ],
      "metadata": {
        "id": "oVu64bDSR8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see in the metadata, even this short clip has 1319 individual frames.\n",
        "When working with an entire television episode, let along a feature-length\n",
        "movie file, it quickly becomes difficult to load all of frames from a video\n",
        "into Python at once. The image sizes, particularly once we start looking at\n",
        "materials in high-definition, are just too large to hold these all in even a\n",
        "computer's memory at the same time.  \n",
        "\n",
        "\n",
        "The distant viewing toolkit (**dvt**) offers an\n",
        "alternative approach using the `yield_video` function. It allows us to write\n",
        "a loop object which loads in each frame of the video file one by one.\n",
        "Specifically, each time the yield function is called, it returns the next frame\n",
        "as an array of pixels, an integer describing the frame number from the start\n",
        "of the video file, and a time code giving the number of seconds since the start\n",
        "of the video.\n",
        "\n",
        "The code below show an example of how the `yield_video` function works. We\n",
        "cycle through each of the frames. For each frame we store the frame number,\n",
        "the timestamp, and the average pixel values (a measurement of the frame's\n",
        "brightness). Then, we turn this into a tabular data frame with one row for\n",
        "each frame in the video."
      ],
      "metadata": {
        "id": "f1j9jGkK_n9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = {'frame': [], 'time': [], 'brightness': []}\n",
        "for img, frame, msec in dvt.yield_video(\"bewitched_sample.mp4\"):\n",
        "  output['frame'].append(frame)\n",
        "  output['time'].append(msec)\n",
        "  output['brightness'].append(np.mean(img))\n",
        "\n",
        "output = pd.DataFrame(output)\n",
        "output"
      ],
      "metadata": {
        "id": "bfisFv5xIRF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brightness is one of the simpliest annotations that we can use to summarize an\n",
        "image. But, even this simple measurement can already provide a rough way of\n",
        "representing our video clip. To see this, let's plot the brightness of the\n",
        "frame over time."
      ],
      "metadata": {
        "id": "He7zGU4YBDKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(output['time'], output['brightness'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2J_og_40IvXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the brightness, particularly the large jumps in the values, can you\n",
        "identify the shot breaks that you found when watching the video? There is a\n",
        "steady increase in the brightness from about 15 seconds to 20 seconds. What does\n",
        "this correspond to in the video file?"
      ],
      "metadata": {
        "id": "S3iKlnAlBYlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Shot Boundary Detection\n",
        "\n",
        "One of the first tasks that we often want to perform on a video file is to\n",
        "identify the breaks between shots. This process is called *shot boundary\n",
        "detection*. As we saw in the brightness plot above, some simple heuristics\n",
        "can be used to approximately identify many kinds of shot breaks. If we want\n",
        "more accurate predictions, which do not falsely identify quick motion as a\n",
        "shot boundary or fail to find subtle breaks such as cross-fades, we need\n",
        "to make use of a more complex algorithm. The distant viewing toolkit (**dvt**) includes\n",
        "a neural-network based shot boundary detection algorithm that we have found\n",
        "works well across many different genres and input types.\n",
        "\n",
        "To run the built-in shot boundary detection algorithm, we need to first create\n",
        "a new annotator with the `AnnoShotBreaks` function. Then, we run the annotator\n",
        "over the video file. This is the only annotator in the toolkit that works\n",
        "directly with a video file rather than individual images or frames. The Python\n",
        "code will print out its progress through the file as it runs, returning a\n",
        "dictionary object with the predicted boundaries."
      ],
      "metadata": {
        "id": "xdfgDX1r1fV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anno_breaks = dvt.AnnoShotBreaks()\n",
        "out_breaks = anno_breaks.run(\"bewitched_sample.mp4\")"
      ],
      "metadata": {
        "id": "AiL-YGOsPPUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert the output of the shot boundary detection into a tabular dataset\n",
        "as well as add the start time and end time information using the metadata that\n",
        "we grabbed from the `video_info` function above. Here is what the algorithm\n",
        "found for this video clip:"
      ],
      "metadata": {
        "id": "i7dHNfZFHjUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shot = pd.DataFrame(out_breaks['scenes'])\n",
        "shot['mid'] = (shot['start'] + shot['end']) // 2\n",
        "shot['start_t'] = shot['start'] / info['fps']\n",
        "shot['end_t'] = shot['end'] / info['fps']\n",
        "shot"
      ],
      "metadata": {
        "id": "rSNg9sEmRxi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a few minutes to confirm that these shot boundaries correspond to the ones\n",
        "that you found when watching the video. You can even rewatch (and pause at each\n",
        "break) the video and see that it corresponds with the breaks that were\n",
        "automatically identified in the shot boundary detection.\n",
        "\n",
        "In the next section we will look at face detection and identification. This is a\n",
        "common task when working with movie image data but ultimately works using\n",
        "individual images. To make this a bit easier to work with at first, we will use\n",
        "the following code to build a dataset with one frame from each of the detected\n",
        "shots in the video clip file. Specifically, we grab the middle frame from each\n",
        "cut, as specified in the `shot` data frame above."
      ],
      "metadata": {
        "id": "SKkvpDvTH1DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = []\n",
        "for img, frame, msec in dvt.yield_video(\"bewitched_sample.mp4\"):\n",
        "  if frame in shot['mid'].values:\n",
        "    img_list.append(img)"
      ],
      "metadata": {
        "id": "4jgWfcwyUTwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a sense of what these look like, we can combine the images and convert\n",
        "them into thumbnails to get a view of what the six shots from our sample video\n",
        "file look like."
      ],
      "metadata": {
        "id": "nP4htqAXKBBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_comb = np.hstack(img_list)\n",
        "img_comb = cv2.resize(img_comb, (img_comb.shape[1] // 5, img_comb.shape[0] // 5))\n",
        "img_comb"
      ],
      "metadata": {
        "id": "JSI2tXpmI4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the full versions of these six images in the next section as we\n",
        "identify the location, size, and identity of the characters in each shot."
      ],
      "metadata": {
        "id": "Gu-O12NPIlLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Face Detection and Facial Recognition\n",
        "\n",
        "Locating and identifying the faces present in a particular frame of a video\n",
        "file is a common way of understanding the narrative structure and visual style\n",
        "of the source material. In this section, we will see how to work with faces\n",
        "using the functions within the distant viewing toolkit (**dvt**) applied to the six\n",
        "images from the sample video file that we extracted in the previous section.\n",
        "After seeing how to do this with a static set of images, in the following\n",
        "section we will see how to put this together in a way that would scale to\n",
        "larger video files.\n",
        "\n",
        "To get started, we will load the face detection annotator `AnnoFaces` that is\n",
        "included in the distant viewing toolkit. The first time this is run, the\n",
        "function will automatically download the relevant model files and save them\n",
        "locally in our Colab workspace."
      ],
      "metadata": {
        "id": "XAhVLHD61jRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anno_face = dvt.AnnoFaces()"
      ],
      "metadata": {
        "id": "snzXIUkHUnlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the annotation, we pass an image file to the `run` method of the\n",
        "annotator. Optionally, we can set the flag `visualize` to `True` in order\n",
        "to return a visualization of the detected face(s). Let's run the annotation\n",
        "over the third image (remembering that Python starts counting at zero, so the\n",
        "third image is selected by selecting the image in position 2)."
      ],
      "metadata": {
        "id": "N-wcCPXZSA9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_face = anno_face.run(img_list[2], visualize=True)"
      ],
      "metadata": {
        "id": "9VOGznnlUv59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we set the visualization flag to `True`, the returned results include\n",
        "an object called `img` that has a copy of the input frame along with a box\n",
        "around any detected faces. Here we see that the algorithm has detected one face,\n",
        "which is likely the number of faces a human annotator would also have found in\n",
        "the frame."
      ],
      "metadata": {
        "id": "8es7DN5qSdUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_face['img']"
      ],
      "metadata": {
        "id": "awm52rz6U36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualization of the detected face is great for a one-off understanding of\n",
        "the algorithm. In order to store this information in a way that we can use for\n",
        "computational analyses, we need to have the location of the detected face\n",
        "represented in a strutured format. This is found in the `boxes` element of the\n",
        "output, which is designed to be convertable into a pandas data frame. Here is\n",
        "all of the structured information about the face detected in the frame above:"
      ],
      "metadata": {
        "id": "ijJUDnvBSurt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(out_face['boxes'])"
      ],
      "metadata": {
        "id": "DG7i7mcqSvA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you followed along with the introductory notebook using movie posters, this\n",
        "is the same information that we used to detect the number of faces present on\n",
        "each poster. One element that makes working with moving images different is that\n",
        "we usually have the same people showing up over and over again across frames and\n",
        "shots. Labeling the identity of the people present in each shot is an essential\n",
        "step in understanding the narrative structure of the material. The distant\n",
        "viewing toolkit (**dvt**) also includes information for doing facial recognition. This\n",
        "requires a bit more care as we need to start by identifying the characters that\n",
        "we want to identify in the first place.\n",
        "\n",
        "By default, when we run the face detection algorithm, each detected faces is\n",
        "also assocaited with a sequence of 512 numbers called an *embedding*. The\n",
        "individual numbers do not have a direct meaning, but are instead defined\n",
        "in a relative way such that if two images of the same person are each associated\n",
        "with an embedding, we would expect the embeddings to be more similar to one\n",
        "another than they are to the embeddings of other faces. As an example, we can\n",
        "see the shape of the embedding object from our example frame above."
      ],
      "metadata": {
        "id": "yeslRhSKTMVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_face['embed'].shape"
      ],
      "metadata": {
        "id": "G5teFubUU-sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to use these embeddings is to first identify portraits of the actors\n",
        "that we are interested in from the video file, and then associate each of these\n",
        "with a character name. Then, we can compute the embeddings of these faces and\n",
        "store them. Finally, each time we detect a face in a frame, we can check how\n",
        "similar the embedding of the detected face is compared to the faces in our\n",
        "reference set. If one of the reference characters is sufficently close, we\n",
        "will assume that we have found the associated character.\n",
        "\n",
        "One of the objects that we downloaded in the setup section of this notebook was\n",
        "a folder with portraits of the four main actors from *Bewitched* named with\n",
        "the name of their character in the show. In the code below we cycle through\n",
        "these images and record the name and embedding of each of the faces. Also,\n",
        "for reference, we store a thumbnail of the actor's portrait."
      ],
      "metadata": {
        "id": "YpHj40NFVJp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_embed = []\n",
        "face_name = []\n",
        "face_img = []\n",
        "for path in sorted(os.listdir(\"faces\")):\n",
        "  img = dvt.load_image(\"faces/\" + path)\n",
        "  out_face = anno_face.run(img)\n",
        "  face_embed += [out_face['embed'][0,:]]\n",
        "  face_name += [path[:-4]]\n",
        "  face_img += [cv2.resize(img, (200, 250))]\n",
        "\n",
        "\n",
        "face_embed = np.vstack(face_embed)\n",
        "face_name = np.array(face_name)\n",
        "face_img = np.hstack(face_img)"
      ],
      "metadata": {
        "id": "58lLg-2Eb4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the portraits of the characters. They are ordered in alphabetical\n",
        "order: Darrin, Endora, Larry, and Sam."
      ],
      "metadata": {
        "id": "qeTzf_-ZWkcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_img"
      ],
      "metadata": {
        "id": "zEy02lH_VafA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to measure the similarity of two embeddings, we use a mathematical\n",
        "technique called a *dot product*. This number will be `1` if two embeddings\n",
        "are exactly the same and `-1` if they are exactly opposite one another.\n",
        "Typically, two faces that are of different people will have a similar score\n",
        "somewhere close to zero. Let's start by seeing the similarity scores between\n",
        "the set of four portraits."
      ],
      "metadata": {
        "id": "mP0Z77pyXyjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(face_embed, face_embed.T)"
      ],
      "metadata": {
        "id": "fS_zyz_qcXEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column is Darrin, and then compares Darrin to each of the four images. Since he is the first image, we see `.99999` because the photo is being\n",
        "compared to itself. Then, image of Darin is compared to Endora (`.06135`),\n",
        "Larry (`-.19993`), and Samantha (`.16960`).\n",
        "\n",
        "The values along the diagonal are all 1 (or very close to one) because we are\n",
        "comparing one embedding to itself. All of the other similarity scores are\n",
        "somewhere between `-0.2` and `+.17`, which is as expected since each of the\n",
        "portraits shows a unique actor.\n",
        "\n",
        "Now, let's compare these reference faces to the embedding of the individual\n",
        "frame that we started this section with: the middle frame from the third shot showing Samantha in a pink bedroom (`img_list[2]`)."
      ],
      "metadata": {
        "id": "XpAm_8gyYb8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_face = anno_face.run(img_list[2])\n",
        "dist = np.dot(face_embed, out_face['embed'].T)\n",
        "dist"
      ],
      "metadata": {
        "id": "LT1M_uPveCvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that there is a much large similarity score (`0.59`) between this\n",
        "face and the last character in our set. And if we look at the image, we see in\n",
        "fact that this is the same character: Samantha.\n",
        "\n",
        " We can associate the face with the\n",
        "character name algorithmically using the following code."
      ],
      "metadata": {
        "id": "7CUzOWYAZmAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_name[np.argmax(dist, axis = 0)]"
      ],
      "metadata": {
        "id": "D15GzlgsduJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above associates the face with the character that has the highest\n",
        "similarity. It would be a good idea to also store the similarity score\n",
        "and then only trust the relationship if this score is sufficently large, which we can do with the following code.\n",
        "\n"
      ],
      "metadata": {
        "id": "bVzJVJSzZ6l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(dist, axis = 0)"
      ],
      "metadata": {
        "id": "pNYIoUIgdz-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand how this process works for a single image, let's apply\n",
        "this technique to each of the shots in the sample file."
      ],
      "metadata": {
        "id": "c7UBEK8laITO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Building Annotations from a Video File\n",
        "\n",
        "We can combine the techniques above with the `yield_video` function from **dvt** to identify\n",
        "faces in each of the shots. It's possible to run the face detection algorithm\n",
        "over every frame, and this has some advantages. For the interest of time\n",
        "and simplicity, we will only use our face detection algorithm on the middle\n",
        "frame of each detected shot."
      ],
      "metadata": {
        "id": "bGHE0yZv1uVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = []\n",
        "for img, frame, msec in dvt.yield_video(\"bewitched_sample.mp4\"):\n",
        "  if frame in shot['mid'].values:\n",
        "    out = anno_face.run(img)\n",
        "    if out['boxes']:\n",
        "      out_df = pd.DataFrame(out['boxes'])\n",
        "      out_df['frame'] = frame\n",
        "      out_df['time'] = msec\n",
        "      dist = np.dot(face_embed, out['embed'].T)\n",
        "      out_df['character'] = face_name[np.argmax(dist, axis = 0)]\n",
        "      out_df['confidence'] = np.max(dist, axis = 0)\n",
        "      output.append(out_df)\n",
        "\n",
        "output = pd.concat(output)\n",
        "output = output[output['prob'] > 0.9]\n",
        "output"
      ],
      "metadata": {
        "id": "dhXsM0hmdkvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a few minutes to go back to the thumbnail images from each of these\n",
        "detected faces and see how they line up with the characters that are actually\n",
        "present (if you are not familiar with *Bewitched*, use the portraits to learn\n",
        "each of the character names). You should see that they line up well, though\n",
        "the fourth shot has a fairly low confidence for Darrin due to the fact that his\n",
        "face is fairly small in the middle frame. We could do better if we added the\n",
        "first frame of the shot, which is more tightly focused on Darrin. We always recommend taking a look and reviewing the results as you go. Then, we can adjust our approach as needed based on the audiovisual data that we are working with and the areas of inquiry animating our analysis.  "
      ],
      "metadata": {
        "id": "8pqoTSvPbEl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Full Corpus Analysis\n",
        "\n",
        "We now have all of the methods and code needed to identify shots and faces in a\n",
        "moving image file. If we had an entire episode of *Bewitched*, we could run the\n",
        "exact same sequence of steps above on the full episode without changing\n",
        "anything. The only difference would be that the results would take much longer\n",
        "to finish running. If we had a folder with every episode of the series, we\n",
        "would just need to add one extra layer to our loop to cycle over each video\n",
        "file and make sure to include information about the filename on each row of\n",
        "the output. Finally, if we wanted to extend this to another series, we would\n",
        "just need to add the portraits of any characters that we wish to identify to\n",
        "our folder of reference images.\n",
        "\n",
        "We return to the two shows — *Bewitched* and *I Dream of Jeanine* — that are central to Chapter 5 of *Distant Viewing*. The chapter centers around a comparison of the two magical sit-coms, which we can do by putting together shot boundary and then face detection and recognition.   \n",
        "\n",
        "The full set of video files for the two shows are quite large and the files are under copyright.\n",
        "It also takes a long time to process all of these files, particularly within a\n",
        "free Colab session. Since it is not possible to run the annotations directly\n",
        "on the full set here, we will instead work with the pre-computed annotations\n",
        "that were downloaded during the notebook setup. The structure of the dataset\n",
        "includes one row for each shot, with data about the timing of the shot and\n",
        "the number of faces."
      ],
      "metadata": {
        "id": "K9nBffuz13Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shots = pd.read_csv(\"ttl_sitcom_shots.csv\")\n",
        "shots"
      ],
      "metadata": {
        "id": "spa2ScRe158V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One quick metric that we can compute is the average shot length for each of the two\n",
        "series. This is a commonly used measurement to understand the pacing of a movie\n",
        "or television show. Here we see that Bewitched is slightly faster, with an\n",
        "average shot length of 5.3 seconds compared to the 6.2 seconds of *I Dream of\n",
        "Jeannie*."
      ],
      "metadata": {
        "id": "3G6jnb7CeEdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shots.groupby(['series'])['time'].mean().sort_values()"
      ],
      "metadata": {
        "id": "EiN5Q-c725Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A related measurement is the median shot length, which here shows that the\n",
        "median shot length is slightly longer in *Bewitched* compared to\n",
        "*Jeannie*. So, while a shot in *Bewitched* is slightly longer, there may\n",
        "be a set of particularly long shows in *Jeannie* that causes the average\n",
        "of that show to be longer.\n",
        "\n"
      ],
      "metadata": {
        "id": "D2iPVVjm7Xz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shots.groupby(['series'])['time'].median().sort_values()"
      ],
      "metadata": {
        "id": "UUIqkwgv3LVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the interesting patterns that we noticed when writting the chapter based\n",
        "on these two series is that the average shot length is closely related to the\n",
        "number of faces present on the screen. We decided to truncate the number of faces to\n",
        "three (so that anything greater than 3 faces becomes 3) given our knowledge of the two shows. Notice how the\n",
        "average shot length increases with the number of faces on screen in both\n",
        "series."
      ],
      "metadata": {
        "id": "AVbUgvrj7v1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shots.loc[shots['num_face'] > 3, 'num_face'] = 3\n",
        "shots.groupby(['series', 'num_face'])['time'].mean()"
      ],
      "metadata": {
        "id": "6FkTmAek35aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw an example of this pattern in our sample video file, where the one shot\n",
        "with two characters was much longer than the shot with a single character. This\n",
        "helps validate that the shot length is related to visual style. At least for\n",
        "these two series, it is related to how often we see a close up on an individual\n",
        "character talking (or acting) versus seeing multiple characters interacting\n",
        "together in a longer shot.\n",
        "\n",
        "We also have a dataset that indicates the specific characters detected in shots\n",
        "from the two series. This uses a face detection algorithm similar to the one\n",
        "that we saw used in our sample video file. Here, we have one row for each\n",
        "detected character in a shot. There are four main characters in each of the\n",
        "series."
      ],
      "metadata": {
        "id": "T5N8TcSg8HWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = pd.read_csv(\"ttl_sitcom_characters.csv\")\n",
        "characters"
      ],
      "metadata": {
        "id": "cIy6QsQdeNW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the average amount of time that a character is on screen in a given\n",
        "episode as a rough measurement of the character's visual importance in the\n",
        "show. Here, we compute this metric for each of the characters and sort in\n",
        "descending order within the series."
      ],
      "metadata": {
        "id": "Bvs4qDLE8sMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = characters.groupby(['series', 'video', 'character'])['time'].agg('sum').reset_index()\n",
        "temp = temp.groupby(['series', 'character'])['time'].agg('mean').reset_index()\n",
        "temp['time'] = temp['time'] / 60\n",
        "temp.sort_values(['series', 'time'], ascending=False)"
      ],
      "metadata": {
        "id": "TeImguuEeWhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, already we see one of the main conclusions of the analysis of these series:\n",
        "while they are thought of as very similar, their narrative structures are\n",
        "quite different. *Jeannie* is actually centered on the male lead Tony; the\n",
        "titual character Jeannie is often little more than a plot device who sets the\n",
        "action in motion. In constrast, *Bewitched* is most focused on the relationship\n",
        "between Samantha and Darrin, which each of them having nearly the same amount\n",
        "of overall screen time.\n",
        "\n",
        "Our annotations - shot detection, face detection, and face recognition - can now be analyzed from many different angles to explore different aspects of the shows, which we delve further into in Chapter 5. Key here is that just three annotations open up analytical possibilities, and can address humanities questions about audiovisual data."
      ],
      "metadata": {
        "id": "XXcq-Uu__qVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 Conclusions and Next Steps\n",
        "\n",
        "This notebook has given an introduction to use the distant viewing toolkit (**dvt**) to\n",
        "annotate movie image data from a digital video file. We've seen how to get\n",
        "basic metadata about a video file from within Python, how to cycle through\n",
        "the individual frames of a video file, and how to detect shot breaks using a\n",
        "custom algorithm. Although not specific to moving images, we also covered the\n",
        "process of using computer vision algorithms to do facial recognition, which is\n",
        "a common task in many applications but particularly useful when working with\n",
        "film and television corpora. Finally, we loaded a larger dataset showing all\n",
        "of the detected shots and characters across the entire runs of two U.S.\n",
        "Network-Era sitcoms and performed several example analyses on them.\n",
        "\n",
        "For readers interested in more details about the specific case study of these\n",
        "two sitcoms, we suggest reading the fifth chapter of the\n",
        "[*Distant Viewing*](https://www.distantviewing.org/book/) book, available\n",
        "under an open access license. The first two chapters of the book may also be\n",
        "of interest as they offer a more general theoretical and methodological\n",
        "approach to the computational analysis of digital images."
      ],
      "metadata": {
        "id": "toEI3DkkAW7X"
      }
    }
  ]
}